# MemoryBench v0.1 Scope

**Version**: 0.1 (Hackathon Demo)
**Status**: In Progress
**Last Updated**: 2025-12-17

## Why This Project Exists

Memory providers for AI applications are fragmented. Developers face:

- **Fragmentation**: Each memory provider has its own API, setup process, and evaluation methodology
- **Slow Setup**: Getting a benchmark running against multiple providers takes significant effort
- **No Standard Comparison**: Hard to objectively compare memory providers without a unified testing framework

MemoryBench solves this by providing a **unified benchmarking platform** where developers can:
1. Add new memory **providers** with a standard interface
2. Add new **benchmarks** to test against
3. Run all combinations via a single CLI

---

## v0.1 Deliverables

The following items must be complete for v0.1 to be considered "done":

| Deliverable | Description | Completion Criterion |
|-------------|-------------|---------------------|
| Unified Runner CLI | Single entry point to run benchmarks | `bun run index.ts --help` shows usage |
| Provider Interface | Standard interface for memory providers | `providers/_template/index.ts` exists with `TemplateType` export |
| Benchmark Interface | Standard interface for benchmarks | `benchmarks/index.ts` exports `BenchmarkRegistry` type |
| Results Writer | Structured output from benchmark runs | Console output shows benchmark results per provider |
| Results Explorer | View and compare results | Results displayed in terminal with provider comparison |
| Documentation | Scope and usage documentation | `docs/v0.1_scope.md` exists (this file) |

### Deliverables Checklist

- [ ] Unified runner CLI accepts `--benchmarks` and `--providers` flags
- [ ] At least 2 providers implemented and registered
- [ ] At least 1 benchmark implemented and registered
- [ ] CLI outputs benchmark results to console
- [ ] README links to scope documentation
- [ ] Demo script runs without errors

---

## v0.1 Targets

### Providers (Target: 2)

| Provider | Status | Location |
|----------|--------|----------|
| ContextualRetrieval | Implemented | `providers/ContextualRetrieval/` |
| AQRAG | Implemented | `providers/AQRAG/` |

### Benchmarks (Target: 1 Primary)

| Benchmark | Status | Location |
|-----------|--------|----------|
| RAG | Primary | `benchmarks/RAG-template-benchmark/` |
| LoCoMo | In Development | `benchmarks/LoCoMo/` |
| LongMemEval | In Development | `benchmarks/LongMemEval/` |

---

## v0.1 Output Artifacts

When a benchmark run completes, the following outputs are produced:

| Artifact | Format | Description | Status |
|----------|--------|-------------|--------|
| Console Output | Text | Real-time progress and results | Implemented |
| run_manifest.json | JSON | Run metadata (timestamp, providers, benchmarks) | Planned |
| results.jsonl | JSONL | Per-item benchmark results | Planned |
| metrics_summary.json | JSON | Aggregated metrics across all items | Planned |
| errors.log | Text | Error log (only if errors occur) | Planned |

---

## Demo Script

### Prerequisites

- [Bun](https://bun.sh) installed (v1.0+)
- Repository cloned locally

### Step 1: Install Dependencies

```bash
bun install
```

### Step 2: Run Baseline Benchmark (Single Provider)

```bash
bun run index.ts --benchmarks RAG-template-benchmark --providers ContextualRetrieval
```

Expected output: Benchmark runs against ContextualRetrieval provider, showing progress and results.

### Step 3: Run Comparison Benchmark (Multiple Providers)

```bash
bun run index.ts --benchmarks RAG-template-benchmark --providers ContextualRetrieval AQRAG
```

Expected output: Benchmark runs against both providers sequentially, allowing comparison of results.

### CLI Reference

```
Usage:
  bun run index.ts --benchmarks <type> --providers <name1> [name2...]

Options:
  --benchmarks, -b    Benchmark types to run (RAG-template-benchmark)
  --providers, -p     Providers to test (ContextualRetrieval, AQRAG)
```

---

## Non-Goals (Out of Scope for v0.1)

The following features are explicitly **not** part of v0.1:

| Feature | Reason | Target Version |
|---------|--------|----------------|
| Web UI | Focus on CLI-first for hackathon | v0.2+ |
| Cloud Deployment | Local development only | v0.2+ |
| Additional Benchmarks | RAG is primary; others in development | v0.2+ |
| Performance Optimizations | Correctness over speed for v0.1 | v0.2+ |
| CI/CD Pipelines | Manual testing sufficient for demo | v0.2+ |
| Structured File Output | Console output sufficient for demo | v0.2+ |
| Provider Authentication | Assumes local/test credentials | v0.2+ |
| Result Persistence | In-memory results for demo | v0.2+ |

---

## Definition of Done

v0.1 is complete when:

1. **All deliverables checked**: Every item in the deliverables checklist is marked complete
2. **Demo script works**: Commands in demo script execute without errors
3. **Documentation exists**: This scope document is complete and linked from README
4. **Two providers functional**: ContextualRetrieval and AQRAG both run successfully
5. **RAG benchmark functional**: RAG benchmark executes and produces results

---

## Maintaining This Document

If CLI flags or commands change:
1. Update the Demo Script section
2. Update the CLI Reference
3. Verify demo commands still work
